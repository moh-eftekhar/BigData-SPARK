{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# Task 1\n",
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input and output folders\n",
    "#inputPath  = \"sampleData/registerSample.csv\" # args[0]\n",
    "#inputPath2 = \"sampleData/stations.csv\" # args[1]\n",
    "#threshold  = 0.4 # args[2]\n",
    "inputPath  = \"/data/students/bigdata-01QYD/Lab7_DBD/register.csv\"\n",
    "inputPath2 = \"/data/students/bigdata-01QYD/Lab7_DBD/stations.csv\"\n",
    "threshold  = 0.6 \n",
    "outputPath = \"out_Lab8SQL\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the standalone version\n",
    "# spark = SparkSession.builder.appName(\"Spark Lab #7 - Template\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of the input file register.csv and store it into a DataFrame\n",
    "# The input file has an header\n",
    "# Schema of the input data:\n",
    "# |-- station: integer (nullable = true)\n",
    "# |-- timestamp: timestamp (nullable = true)\n",
    "# |-- used_slots: integer (nullable = true)\n",
    "# |-- free_slots: integer (nullable = true)\n",
    "inputDF = spark.read.format(\"csv\")\\\n",
    ".option(\"delimiter\", \"\\\\t\")\\\n",
    ".option(\"header\", True)\\\n",
    ".option(\"inferSchema\", True).load(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the “table name” readings to inputDF\n",
    "inputDF.createOrReplaceTempView(\"readings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a User Defined Function called full(Integer free_slots)\n",
    "# that returns \n",
    "# -- 1 if free_slots is equal to 0\n",
    "# -- 0 if free_slots is greater than 0\n",
    "def fullFunction(free_slots):\n",
    "    if free_slots==0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the UDF\n",
    "# name: full\n",
    "# output: integer value\n",
    "spark.udf.register(\"full\", fullFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the lines with free_slots<>0 or used_slots<>0 and then \n",
    "# compute the criticality for each group (station, dayofweek, hour) (i.e., for each pair (station, timeslot))\n",
    "# and finally select only the groups with criticality>threshold.\n",
    "#\n",
    "# The criticality of each group is equal to the average of full(free_slots)\n",
    "# The schema of the returned dataset is:\n",
    "# |-- station: integer (nullable = true)\n",
    "# |-- dayofweek: string (nullable = true)\n",
    "# |-- hour: integer (nullable = true)\n",
    "# |-- criticality: double (nullable = true)\n",
    "selectedPairsDF = spark.sql(\"\"\"SELECT station, date_format(timestamp,'EE') as dayofweek, \n",
    "hour(timestamp) as hour, avg(full(free_slots)) as criticality \n",
    "FROM readings \n",
    "WHERE free_slots<>0 OR used_slots<>0\n",
    "GROUP BY station, date_format(timestamp,'EE'), hour(timestamp)\n",
    "HAVING avg(full(free_slots))>\"\"\"+str(threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the “table name” criticals to selectedPairsDF\n",
    "selectedPairsDF.createOrReplaceTempView(\"criticals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of the input file stations.csv and store it into a DataFrame\n",
    "# The input file has an header\n",
    "# Schema of the input data:\n",
    "# |-- id: integer (nullable = true)\n",
    "# |-- longitude: double (nullable = true)\n",
    "# |-- latitude: double (nullable = true)\n",
    "# |-- name: string (nullable = true)\n",
    "stationsDF = spark.read.format(\"csv\")\\\n",
    ".option(\"delimiter\", \"\\\\t\")\\\n",
    ".option(\"header\", True)\\\n",
    ".option(\"inferSchema\", True).load(inputPath2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the “table name” stations to stationsDF\n",
    "stationsDF.createOrReplaceTempView(\"stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the selected critical \"situations\" with the stations table to\n",
    "# retrieve the coordinates of the stations.\n",
    "# Select only the column station, longitude, latitude and criticality\n",
    "# and sort records by criticality (desc), station (asc)\n",
    "selectedPairsIdCoordinatesCriticalityDF = spark.sql(\"\"\"SELECT station, dayofweek, hour, \n",
    "longitude, latitude, criticality\n",
    "FROM criticals, stations\n",
    "WHERE criticals.station = stations.id\n",
    "ORDER BY criticality DESC, station, dayofweek, hour\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedPairsIdCoordinatesCriticalityDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedPairsIdCoordinatesCriticalityDF.write.format(\"csv\")\\\n",
    ".option(\"header\", True)\\\n",
    ".save(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
