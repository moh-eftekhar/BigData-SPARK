{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# Task 1\n",
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input and output folders\n",
    "#inputPath  = \"sampleData/registerSample.csv\"\n",
    "#inputPath2 = \"sampleData/stations.csv\" \n",
    "#threshold  = 0.4 \n",
    "inputPath  = \"/data/students/bigdata-01QYD/Lab7_DBD/register.csv\" \n",
    "inputPath2 = \"/data/students/bigdata-01QYD/Lab7_DBD/stations.csv\" \n",
    "threshold  = 0.6\n",
    "outputPath = \"out_Lab8DF\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the standalone version\n",
    "# spark = SparkSession.builder.appName(\"Spark Lab #7 - Template\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of the input file register.csv and store it into a DataFrame\n",
    "# The input file has an header\n",
    "# Schema of the input data:\n",
    "# |-- station: integer (nullable = true)\n",
    "# |-- timestamp: timestamp (nullable = true)\n",
    "# |-- used_slots: integer (nullable = true)\n",
    "# |-- free_slots: integer (nullable = true)\n",
    "inputDF = spark.read.format(\"csv\")\\\n",
    ".option(\"delimiter\", \"\\\\t\")\\\n",
    ".option(\"header\", True)\\\n",
    ".option(\"inferSchema\", True).load(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the lines with num free slots=0 && num used slots=0\n",
    "filteredDF = inputDF.filter(\"free_slots<>0 OR used_slots<>0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a User Defined Function called full(Integer free_slots)\n",
    "# that returns \n",
    "# -- 1 if free_slots is equal to 0\n",
    "# -- 0 if free_slots is greater than 0\n",
    "def fullFunction(free_slots):\n",
    "    if free_slots==0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the UDF\n",
    "# name: full\n",
    "# output: integer value\n",
    "spark.udf.register(\"full\", fullFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a DataFrame with the following schema:\n",
    "# |-- station: integer (nullable = true)\n",
    "# |-- dayofweek: string (nullable = true)\n",
    "# |-- hour: integer (nullable = true)\n",
    "# |-- fullstatus: integer (nullable = true) - 1 = full, 0 = non-full\n",
    "stationWeekdayHourDF = filteredDF.selectExpr(\"station\", \"date_format(timestamp,'EE') as dayofweek\",\\\n",
    "                                             \"hour(timestamp) as hour\", \"full(free_slots) as fullstatus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one group for each combination \"station, dayofweek, hour\"\n",
    "rgdStationWeekdayHourDF = stationWeekdayHourDF.groupBy(\"station\", \"dayofweek\", \"hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the criticality for each group (station, dayofweek, hour),\n",
    "# i.e., for each pair (station, timeslot)\n",
    "# The criticality is equal to the average of fullStatus\n",
    "stationWeekdayHourCriticalityDF = rgdStationWeekdayHourDF.agg({\"fullStatus\": \"avg\"})\\\n",
    ".withColumnRenamed(\"avg(fullStatus)\", \"criticality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the lines with criticality > threshold\n",
    "selectedPairsDF = stationWeekdayHourCriticalityDF.filter(\"criticality>\"+str(threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of the input file stations.csv and store it into a DataFrame\n",
    "# The input file has an header\n",
    "# Schema of the input data:\n",
    "# |-- id: integer (nullable = true)\n",
    "# |-- longitude: double (nullable = true)\n",
    "# |-- latitude: double (nullable = true)\n",
    "# |-- name: string (nullable = true)\n",
    "stationsDF = spark.read.format(\"csv\")\\\n",
    ".option(\"delimiter\", \"\\\\t\")\\\n",
    ".option(\"header\", True)\\\n",
    ".option(\"inferSchema\", True).load(inputPath2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the selected critical \"situations\" with the stations table to\n",
    "# retrieve the coordinates of the stations\n",
    "selectedPairsCoordinatesDF = selectedPairsDF.join(stationsDF,\\\n",
    "                                                  selectedPairsDF.station == stationsDF.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the content of the DataFrame\n",
    "selectedPairsIdCoordinatesCriticalityDF = selectedPairsCoordinatesDF\\\n",
    ".selectExpr(\"station\", \"dayofweek\", \"hour\", \"longitude\", \"latitude\", \"criticality\")\\\n",
    ".sort(selectedPairsCoordinatesDF.criticality.desc(),\\\n",
    "      selectedPairsCoordinatesDF.station,\\\n",
    "      selectedPairsCoordinatesDF.dayofweek,\\\n",
    "      selectedPairsCoordinatesDF.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedPairsIdCoordinatesCriticalityDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedPairsIdCoordinatesCriticalityDF.write.format(\"csv\")\\\n",
    ".option(\"header\", True)\\\n",
    ".save(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
